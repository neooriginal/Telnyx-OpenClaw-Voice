require("dotenv").config();
const express = require("express");
const path = require("path");
const fs = require("fs");
const telnyxService = require("./telnyxService");
const stateManager = require("./stateManager");
const openaiService = require("./openaiService");
const ngrok = require("@ngrok/ngrok");

const app = express();
const port = process.env.PORT || 3000;
let baseUrl = process.env.BASE_URL;

// Middleware to parse raw body for signature verification if needed
// Or standard JSON parsing
app.use(express.json());

// Serve static audio files generated by TTS
app.use("/audio", express.static(path.join(__dirname, "audio")));

// Ensure audio directory exists
const audioDir = path.join(__dirname, "audio");
if (!fs.existsSync(audioDir)) {
    fs.mkdirSync(audioDir);
}

// Inbound webhook from Telnyx
app.post("/voice/webhook", async (req, res) => {
    // Acknowledge the webhook quickly
    res.status(200).send("OK");

    try {
        const event = req.body;
        if (!event || !event.data || !event.data.event_type) return;

        const eventType = event.data.event_type;
        const payload = event.data.payload;
        const callControlId = payload.call_control_id;

        console.log(`Received event: ${eventType} for call ${callControlId}`);

        if (eventType === "call.initiated") {
            // Only care about inbound calls
            if (payload.direction === "incoming") {
                await telnyxService.answerCall(callControlId);
                stateManager.initSession(callControlId);
            }
        } else if (eventType === "call.answered") {
            // Generate a greeting
            const greeting = "Hello, I am an AI assistant. How can I help you today?";
            stateManager.addMessage(callControlId, { role: "assistant", content: greeting });

            const filename = `greeting_${callControlId}.mp3`;
            const audioPath = path.join(audioDir, filename);
            await openaiService.generateTTS(greeting, audioPath);

            // Play audio to caller
            const audioUrl = `${baseUrl}/audio/${filename}`;
            console.log(`Playing audio: ${audioUrl}`);
            await telnyxService.playAudio(callControlId, audioUrl);

        } else if (eventType === "call.playback.ended" || eventType === "call.speak.ended") {
            // Audio finished playing. Start recording user speech.
            console.log(`Playback/Speak ended. Starting record for call ${callControlId}`);
            await telnyxService.recordAudio(callControlId);

        } else if (eventType === "call.recording.saved") {
            // User finished speaking and recording is saved
            console.log(`Recording saved for call ${callControlId}. Processing...`);
            const recordingUrl = payload.recording_urls.mp3;

            // 1. Download recording
            const localRecordingPath = path.join(audioDir, `user_${callControlId}_${Date.now()}.mp3`);
            await telnyxService.downloadRecording(recordingUrl, localRecordingPath);

            // 2. Transcribe
            const transcript = await openaiService.transcribeAudio(localRecordingPath);
            console.log(`User transcript: ${transcript}`);

            if (!transcript || transcript.trim().length === 0) {
                // Try recording again
                await telnyxService.recordAudio(callControlId);
                return;
            }

            // 3. Get LLM response
            stateManager.addMessage(callControlId, { role: "user", content: transcript });
            const messages = stateManager.getMessages(callControlId);

            const aiResponse = await openaiService.getChatCompletion(messages);
            stateManager.addMessage(callControlId, { role: "assistant", content: aiResponse });
            console.log(`AI Response: ${aiResponse}`);

            // 4. Generate TTS for response
            const ttsFilename = `response_${callControlId}_${Date.now()}.mp3`;
            const ttsPath = path.join(audioDir, ttsFilename);
            await openaiService.generateTTS(aiResponse, ttsPath);

            // 5. Play it back
            const ttsUrl = `${baseUrl}/audio/${ttsFilename}`;
            await telnyxService.playAudio(callControlId, ttsUrl);

        } else if (eventType === "call.hangup") {
            console.log(`Call hung up: ${callControlId}`);
            stateManager.endSession(callControlId);
        } else if (eventType === "call.transcription" || eventType === "call.speak.started" || eventType === "call.playback.started") {
            // Ignore for now to keep logs clean
        } else {
            console.log(`Unhandled event type: ${eventType}`);
        }

    } catch (err) {
        console.error("Error processing webhook:", err);
    }
});

app.listen(port, async () => {
    console.log(`Server listening on port ${port}`);

    // Optional ngrok tunneling
    if (process.env.NGROK_AUTHTOKEN) {
        try {
            const session = await new ngrok.SessionBuilder()
                .authtokenFromEnv()
                .metadata("Telnyx Voice AI App")
                .connect();

            const tunnelBuilder = session.httpEndpoint();
            if (process.env.NGROK_DOMAIN) {
                tunnelBuilder.domain(process.env.NGROK_DOMAIN);
            }

            const tunnel = await tunnelBuilder.listen();
            console.log(`ngrok tunnel established at: ${tunnel.url()}`);

            // Update baseUrl for Telnyx callback logic
            baseUrl = tunnel.url();
        } catch (err) {
            console.error("Error starting ngrok tunnel:", err);
        }
    } else {
        console.log(`Base URL: ${baseUrl}`);
    }
});
